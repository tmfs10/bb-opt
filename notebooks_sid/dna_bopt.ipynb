{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/cluster/sj1')\n",
    "sys.path.append('/cluster/sj1/bb_opt/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU 1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from collections import namedtuple\n",
    "import torch.distributions as tdist\n",
    "import bb_opt.src.reparam_trainer as reparam\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bb_opt.src.dna_bopt as dbopt\n",
    "from gpu_utils.utils import gpu_init\n",
    "from tqdm import tnrange\n",
    "import pandas as pd\n",
    "from bb_opt.src.utils import train_val_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils\n",
    "\n",
    "gpu_id = gpu_init()\n",
    "print(f\"Running on GPU {gpu_id}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Params = namedtuple('params', [\n",
    "    'lr', \n",
    "    'num_latents', \n",
    "    'output_dist_std', \n",
    "    'output_dist_fn', \n",
    "    'prior_mean', \n",
    "    'prior_std', \n",
    "    'num_epochs', \n",
    "    'num_samples', \n",
    "    'batch_size', \n",
    "    'device', \n",
    "    'exp_noise_samples'])\n",
    "params = Params(\n",
    "    batch_size=100, \n",
    "    num_latents=20,\n",
    "    output_dist_std=0.01,\n",
    "    output_dist_fn=tdist.Normal, \n",
    "    num_samples=10, \n",
    "    exp_noise_samples=2, \n",
    "    lr=1e-3, \n",
    "    prior_mean=0., \n",
    "    prior_std=1., \n",
    "    device='cuda', \n",
    "    num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "\n",
    "project = \"dna_binding\"\n",
    "dataset = \"crx_ref_r1\"\n",
    "\n",
    "root = \"/cluster/sj1/bb_opt/\"\n",
    "data_dir = root+\"data/\"+project+\"/\"+dataset+\"/\"\n",
    "inputs = np.load(data_dir+\"inputs.npy\")\n",
    "labels = np.load(data_dir+\"labels.npy\")\n",
    "\n",
    "\n",
    "exclude_top = 0.1\n",
    "\n",
    "idx = np.arange(labels.shape[0])\n",
    "\n",
    "sort_idx = labels.argsort()[:-int(labels.shape[0]*exclude_top)]\n",
    "idx = idx[sort_idx]\n",
    "\n",
    "train_idx, _, _ = train_val_test_split(idx, split=[n_train, 0])\n",
    "train_idx2, _, test_idx2 = train_val_test_split(n_train, split=[0.9, 0])\n",
    "\n",
    "test_idx = train_idx[test_idx2]\n",
    "train_idx = train_idx[train_idx2]\n",
    "\n",
    "train_inputs = inputs[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "\n",
    "val_inputs = inputs[test_idx]\n",
    "val_labels = labels[test_idx]\n",
    "\n",
    "\n",
    "#train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, train_size=n_train, random_state=521)\n",
    "#train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, train_size=0.9, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_label_mean = train_labels.mean()\n",
    "train_label_std = train_labels.std()\n",
    "\n",
    "train_labels = (train_labels - train_label_mean) / train_label_std\n",
    "val_labels = (val_labels - train_label_mean) / train_label_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bb_opt.src.dna_bopt as dbopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = Params(\n",
    "    batch_size=10,\n",
    "    num_epochs=500,\n",
    "    num_latents=15, \n",
    "    output_dist_fn=tdist.Normal, \n",
    "    num_samples=20, \n",
    "    exp_noise_samples=3, \n",
    "    lr=1e-4, \n",
    "    prior_mean=0., \n",
    "    prior_std=3.,\n",
    "    device='cuda', \n",
    "    output_dist_std=1.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches: 90\n"
     ]
    }
   ],
   "source": [
    "model, qz, e_dist = dbopt.get_model_nn(inputs.shape[1], params.num_latents, params.prior_std)\n",
    "\n",
    "train_losses = []\n",
    "train_kl_losses = []\n",
    "train_hsic_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_corrs = []\n",
    "val_corrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = torch.FloatTensor(train_inputs, device='cpu')\n",
    "train_Y = torch.FloatTensor(train_labels, device='cpu')\n",
    "val_X = torch.FloatTensor(val_inputs, device='cpu')\n",
    "val_Y = torch.FloatTensor(val_labels, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bb_opt.src.dna_bopt as dbopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [train_X, train_Y, val_X, val_Y]\n",
    "logging = dbopt.train(params, params.num_train_latent_samples, data, model, qz, e_dist)\n",
    "i_losses, i_kl_losses, i_hsic_losses, i_val_losses, i_corrs, i_val_corrs = logging\n",
    "\n",
    "train_losses += i_losses\n",
    "train_kl_losses += i_kl_losses\n",
    "train_hsic_losses += i_hsic_losses\n",
    "val_losses += i_val_losses\n",
    "\n",
    "train_corrs += i_corrs\n",
    "val_corrs += i_val_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"log prob loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_losses[-1000:])\n",
    "plt.title(\"Recent log prob loss\")\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_kl_losses)\n",
    "plt.title(\"kl loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_kl_losses[-3000:])\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(train_hsic_losses)\n",
    "plt.title(\"hsic loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_hsic_losses[-3000:])\n",
    "\n",
    "plt.title(\"Recent hsic loss\")\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(train_corrs, label=\"train_corrs\")\n",
    "plt.plot(val_corrs, label=\"val_corrs\")\n",
    "plt.legend()\n",
    "plt.title(\"Kendall Tau\");\n",
    "\n",
    "title = \"DNA Binding - CRX\"\n",
    "train_title = title + \" (train)\"\n",
    "val_title = title + \" (val)\"\n",
    "\n",
    "if n_train > 1:\n",
    "    preds = reparam.predict(train_X, model, qz, e)[:, :, 0].mean(1)\n",
    "    jointplot(preds, train_labels, train_title)\n",
    "    print('train_corrcoef:', np.corrcoef(preds, train_labels)[0, 1])\n",
    "\n",
    "preds = reparam.predict(val_X, model, qz, e)[:, :, 0].mean(1)\n",
    "jointplot(preds, val_labels, val_title)\n",
    "print('val_corrcoef:', np.corrcoef(preds, val_labels)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(inputs, device=device)\n",
    "Y = torch.tensor(labels, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bayesian_opt as bopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = reparam.generate_prior_samples(params.ack_num_model_samples, e_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_idx = set(train_idx)\n",
    "for ack_iter in range(10):\n",
    "    model_ensemble = reparam.generate_ensemble_from_stochastic_net(model, e)\n",
    "    preds = model_ensemble(X, resize_at_end=True) # (num_candidate_points, num_samples)\n",
    "    preds = preds.tranpose(0, 1)\n",
    "    \n",
    "    ei = preds.mean(dim=0).view(-1).cpu().numpy()\n",
    "    ei_sortidx = np.argsort(ei)\n",
    "    \n",
    "    ack_ei = X[ei_sortidx[-params.ack_batch_size:]]\n",
    "    \n",
    "    train_X = torch.cat([train_X, ack_ei], dim=0)\n",
    "    data = [train_X, train_Y, val_X, val_Y]\n",
    "    logging = dbopt.train(params, params.num_train_latent_samples, data, model, qz, e_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_idx = set(train_idx)\n",
    "for ack_iter in range(10):\n",
    "    preds = model_ensemble(X, resize_at_end=True) # (num_candidate_points, num_samples)\n",
    "    preds = preds.tranpose(0, 1)\n",
    "    \n",
    "    max_pred = preds.max(dim=1).view(-1)\n",
    "    mves_idx = dbopt.acquire_batch_mves_sid(params, max_pred, preds, params.mves_compute_batch_size)\n",
    "    skip_idx.update(mves_idx)\n",
    "    \n",
    "    ack_mves = X[torch.tensor(mves_idx, device=params.device)]\n",
    "    \n",
    "    train_X = torch.cat([train_X, ack_mves], dim=0)\n",
    "    data = [train_X, train_Y, val_X, val_Y]\n",
    "    logging = dbopt.train(params, params.num_train_latent_samples, data, model, qz, e_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ack_iter in range(10):\n",
    "    preds = model_ensemble(X, resize_at_end=True) # (num_candidate_points, num_samples)\n",
    "    preds = preds.tranpose(0, 1)\n",
    "    \n",
    "    max_pred_idx = preds.argmax(dim=1).view(-1)\n",
    "    max_pred = preds.max(dim=1).view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
