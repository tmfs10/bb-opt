{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/cluster/sj1/bb_opt/src')\n",
    "sys.path.append('/cluster/sj1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU 0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "from collections import namedtuple\n",
    "import torch.distributions as tdist\n",
    "from gpu_utils.utils import gpu_init\n",
    "from tqdm import tnrange\n",
    "import pandas as pd\n",
    "from bb_opt.src.utils import train_val_test_split\n",
    "\n",
    "gpu_id = gpu_init()\n",
    "print(f\"Running on GPU {gpu_id}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import hsic\n",
    "import bayesian_opt as bopt\n",
    "import chemvae_bopt as cbopt\n",
    "import reparam_trainer as reparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chemvae_keras import vae_utils\n",
    "from chemvae_keras import mol_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chemvae_num_z = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = namedtuple('params', [\n",
    "    'lr', \n",
    "    'output_dist_std', \n",
    "    'output_dist_fn',\n",
    "    'prior_mean', \n",
    "    'prior_std', \n",
    "    'num_epochs', \n",
    "    'num_samples', \n",
    "    'train_batch_size', \n",
    "    'device', \n",
    "    'exp_noise_samples',\n",
    "    'hsic_train_lambda',\n",
    "    \n",
    "    'prop_activation',\n",
    "    'prop_pred_num_input_features',\n",
    "    'prop_pred_num_random_inputs',\n",
    "    'prop_pred_num_hidden',\n",
    "    'prop_pred_dropout',\n",
    "    'prop_pred_depth',\n",
    "    'prop_pred_growth_factor',\n",
    "    'prop_batchnorm',\n",
    "    \n",
    "    'ack_batch_size',\n",
    "    'num_queries',\n",
    "    'batch_opt_lr',\n",
    "    'batch_opt_num_iter',\n",
    "    'input_opt_lr',\n",
    "    'mves_kernel_fn',\n",
    "    'input_opt_num_iter',\n",
    "    'ack_num_model_samples',\n",
    "    'hsic_diversity_lambda',\n",
    "    \n",
    "    'retrain_iters',\n",
    "    \n",
    "    'score_fn', # order is logp, qed, sas\n",
    "])\n",
    "\n",
    "params = Params(\n",
    "    train_batch_size=100, \n",
    "    output_dist_std=0.01,\n",
    "    output_dist_fn=tdist.Normal, \n",
    "    num_samples=10, \n",
    "    exp_noise_samples=2, \n",
    "    lr=1e-3, \n",
    "    prior_mean=0., \n",
    "    prior_std=1., \n",
    "    device='cuda', \n",
    "    num_epochs=1000,\n",
    "    hsic_train_lambda=20.,\n",
    "    \n",
    "    prop_activation='relu',\n",
    "    prop_pred_num_input_features=chemvae_num_z,\n",
    "    prop_pred_num_random_inputs=20,\n",
    "    prop_pred_num_hidden=50,\n",
    "    prop_pred_dropout=False,\n",
    "    prop_pred_depth=2,\n",
    "    prop_pred_growth_factor=1.5,\n",
    "    prop_batchnorm=True,\n",
    "    \n",
    "    ack_batch_size=5,\n",
    "    num_queries=1,\n",
    "    batch_opt_lr=1e-3,\n",
    "    batch_opt_num_iter=100,\n",
    "    input_opt_lr=1e-3,\n",
    "    mves_kernel_fn='mixrq_kernels',\n",
    "    input_opt_num_iter=100,\n",
    "    ack_num_model_samples=100,\n",
    "    hsic_diversity_lambda=1.,\n",
    "    \n",
    "    retrain_iters=100,\n",
    "    \n",
    "    score_fn=lambda x : x[0],\n",
    ")\n",
    "\n",
    "device = params.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesian_opt as bopt\n",
    "import chemvae_bopt as cbopt\n",
    "import reparam_trainer as reparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read zinc250k from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/sj1/anaconda2/envs/py36/lib/python3.6/site-packages/keras/models.py:258: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standarized functions? True\n",
      "Standarization: estimating mu and std values ...done!\n",
      "initialized VAEUtils\n",
      "done 0K samples\n",
      "converted to one_hot\n",
      "(10000, 196)\n",
      "processed encoded representation\n"
     ]
    }
   ],
   "source": [
    "zinc250k, vae, smiles_one_hot, smiles_z, labels = cbopt.load_zinc250k(score_fn=params.score_fn, num_to_load=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_top = 0.01\n",
    "n_train = 1000\n",
    "random_selection = False\n",
    "\n",
    "num_total = len(zinc250k[0])\n",
    "idx = np.arange(num_total)\n",
    "\n",
    "if random_selection:\n",
    "    train_idx, _, _ = train_val_test_split(idx, split=[n_train, 0])\n",
    "    train_idx2, _, test_idx2 = train_val_test_split(n_train, split=[0.9, 0])\n",
    "\n",
    "    test_idx = train_idx[test_idx2]\n",
    "    train_idx = train_idx[train_idx2]\n",
    "else:\n",
    "    num_train = int(0.9 * n_train)    \n",
    "    train_idx = idx[:num_train]\n",
    "    test_idx = idx[num_train:n_train]\n",
    "    \n",
    "\n",
    "train_inputs = inputs[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "val_inputs = inputs[test_idx]\n",
    "val_labels = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label_mean = train_labels.mean()\n",
    "train_label_std = train_labels.std()\n",
    "\n",
    "train_labels = (train_labels - train_label_mean) / train_label_std\n",
    "val_labels = (val_labels - train_label_mean) / train_label_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = cbopt.PropertyPredictor(params)\n",
    "qz = reparam.GaussianQz(params.prop_pred_num_random_inputs)\n",
    "\n",
    "mu_e = torch.zeros(params.prop_pred_num_random_inputs, requires_grad=False).to(device)\n",
    "std_e = torch.ones(params.prop_pred_num_random_inputs, requires_grad=False).to(device)\n",
    "\n",
    "e_dist = tdist.Normal(mu_e + params.prior_mean, std_e*params.prior_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_kl_losses = []\n",
    "train_hsic_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_corrs = []\n",
    "val_corrs = []\n",
    "\n",
    "train_X = torch.FloatTensor(train_inputs)\n",
    "train_Y = torch.FloatTensor(train_labels)\n",
    "val_X = torch.FloatTensor(val_inputs)\n",
    "val_Y = torch.FloatTensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [train_X, train_Y, val_X, val_Y]\n",
    "i_losses, i_kl_losses, i_hsic_losses, i_val_losses, i_corrs, i_val_corrs = train(params, data, model, qz, e_dist)\n",
    "\n",
    "train_losses = i_losses\n",
    "train_kl_losses = i_kl_losses\n",
    "train_hsic_losses = i_hsic_losses\n",
    "val_losses = i_val_losses\n",
    "\n",
    "train_corrs = i_corrs\n",
    "val_corrs = i_val_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"log prob loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_losses[-1000:])\n",
    "plt.title(\"Recent log prob loss\")\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_kl_losses)\n",
    "plt.title(\"kl loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_kl_losses[-3000:])\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.plot(train_hsic_losses)\n",
    "plt.title(\"hsic loss\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_hsic_losses[-3000:])\n",
    "\n",
    "plt.title(\"Recent hsic loss\")\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.plot(train_corrs, label=\"train_corrs\")\n",
    "plt.plot(val_corrs, label=\"val_corrs\")\n",
    "plt.legend()\n",
    "plt.title(\"Kendall Tau\");\n",
    "\n",
    "title = \"DNA Binding - CRX\"\n",
    "train_title = title + \" (train)\"\n",
    "val_title = title + \" (val)\"\n",
    "\n",
    "if n_train > 1:\n",
    "    preds = reparam.predict(train_X, model, qz, e)[:, :, 0].mean(1)\n",
    "    jointplot(preds, train_labels, train_title)\n",
    "    print('train_corrcoef:', np.corrcoef(preds, train_labels)[0, 1])\n",
    "\n",
    "preds = reparam.predict(val_X, model, qz, e)[:, :, 0].mean(1)\n",
    "jointplot(preds, val_labels, val_title)\n",
    "print('val_corrcoef:', np.corrcoef(preds, val_labels)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = reparam.generate_prior_samples(params.ack_num_model_samples, e_dist)\n",
    "model_ensemble = reparam.generate_ensemble_from_stochastic_net(model, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = model.input_shape()\n",
    "\n",
    "for ack_iter in range(params.num_queries)\n",
    "    good_points = bopt.optimize_model_input(\n",
    "        params,\n",
    "        input_shape, \n",
    "        model_ensemble, \n",
    "        hsic_diversity_lambda=params.hsic_diversity_lambda\n",
    "    )\n",
    "    ei_batch = bopt.acquire_batch_via_grad_ei(\n",
    "        params,\n",
    "        model_ensemble,\n",
    "        input_shape\n",
    "    )\n",
    "    mves_batch = bopt.acquire_batch_via_grad_mves(\n",
    "        params,\n",
    "        model_ensemble,\n",
    "        input_shape\n",
    "    )\n",
    "    \n",
    "    ack_props = []\n",
    "    ack_props += [cbopt.acquire_properties(good_points, vae, params.device)]\n",
    "    ack_props += [cbopt.acquire_properties(ei_batch, vae, params.device)]\n",
    "    ack_props += [cbopt.acquire_properties(mves_batch, vae, params.device)]\n",
    "    \n",
    "    ack_labels = []\n",
    "    for i in range(len(ack_props)):\n",
    "        new_labels = [params.score_fn(ack_props[i][j]) for j in ack_props[i].shape[0]]\n",
    "        ack_labels += [torch.tensor(new_labels)]\n",
    "    ack_labels = torch.tensor(ack_labels)\n",
    "    assert ack_labels.ndimension() == 2\n",
    "    max_ack = torch.max(ack_labels, dim=1)\n",
    "    \n",
    "    print(max_ack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
